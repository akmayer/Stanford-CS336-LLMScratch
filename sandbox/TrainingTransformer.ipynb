{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8706c703-d8ae-4438-878e-bf514465e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67380070-1dfd-42b5-af76-4a2c347dc6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1587, 0.0488, 0.4874, 0.2383, 0.0668],\n",
       "        [0.2220, 0.1276, 0.0781, 0.2395, 0.3328]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from cs336_basics.transformerStuff import softmax\n",
    "\n",
    "exampleLogits = torch.randn(2, 5)\n",
    "exampleIndices = torch.tensor([0, 2])\n",
    "\n",
    "probs = softmax(exampleLogits, dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cb4cc05-a255-4123-808e-4bdf5f2ecb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einx\n",
    "trueLogits = einx.get_at(\"b [v], (b [idx]) -> b 1\", exampleLogits, exampleIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62c2f985-a1bc-452a-8524-defbbe6b3278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7186],\n",
       "        [1.1003]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eLogits = torch.exp(exampleLogits - exampleLogits.max(dim=-1, keepdim=True).values)\n",
    "logSumExp = torch.log(eLogits.sum(axis=-1, keepdim=True))\n",
    "logSumExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee609f1d-b453-4c00-8a63-637539717511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2175)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(- (trueLogits - logSumExp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "469c48f4-8a8b-425d-9fab-e1aebc3232ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(inputs, targets):\n",
    "    \"\"\"\n",
    "        Given a tensor of inputs and targets, compute the average cross-entropy\n",
    "        loss across examples.\n",
    "    \n",
    "        Args:\n",
    "            inputs (Float[Tensor, \"batch_size vocab_size\"]): inputs[i][j] is the\n",
    "                unnormalized logit of jth class for the ith example.\n",
    "            targets (Int[Tensor, \"batch_size\"]): Tensor of shape (batch_size,) with the index of the correct class.\n",
    "                Each value must be between 0 and `num_classes - 1`.\n",
    "    \n",
    "        Returns:\n",
    "            Float[Tensor, \"\"]: The average cross-entropy loss across examples.\n",
    "    \"\"\"\n",
    "    inputs = einx.rearrange(\"... v -> (...) v\", inputs)\n",
    "    targets = einx.rearrange(\"... -> (...)\", targets)\n",
    "    inputs = inputs - inputs.max(dim=-1, keepdim=True).values\n",
    "    trueLogits = einx.get_at(\"b [v], (b [idx]) -> b 1\", inputs, targets)\n",
    "    eLogits = torch.exp(inputs)\n",
    "    logSumExp = torch.log(eLogits.sum(axis=-1, keepdim=True))\n",
    "    return torch.mean(- (trueLogits - logSumExp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dcdf1122-28e7-4acc-bb3e-9369cbc157c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1950)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(exampleLogits, exampleIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0fe8d2e7-7358-4b17-b0c0-a2664bdeac96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6095)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs =  torch.tensor([[0.1088, 0.1060, 0.6683, 0.5131, 0.0645],\n",
    "               [0.4538, 0.6852, 0.2520, 0.3792, 0.2675],\n",
    "               [0.4578, 0.3357, 0.6384, 0.0481, 0.5612],\n",
    "               [0.9639, 0.8864, 0.1585, 0.3038, 0.0350],\n",
    "               [0.3356, 0.9013, 0.7052, 0.8294, 0.8334],\n",
    "              [0.6333, 0.4434, 0.1428, 0.5739, 0.3810],\n",
    "              [0.9476, 0.5917, 0.7037, 0.2987, 0.6208],\n",
    "               [0.8541, 0.1803, 0.2054, 0.4775, 0.8199]])\n",
    "targets = torch.tensor([1, 0, 2, 2, 4, 1, 4, 0])\n",
    "cross_entropy(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b302f32-2881-4bcf-8605-11f8a41b9470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0685)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.randn(3, 5, 7)\n",
    "targets = torch.randint(0, 7, size=(3, 5))\n",
    "cross_entropy(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa3495dd-672e-4ff8-918d-a57e72acbccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Iterable\n",
    "from typing import Optional\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d5d2a8e6-1ca1-472c-9b0c-e41e5236b2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "29.319351196289062\n",
      "28.158300399780273\n",
      "27.36750030517578\n",
      "26.739124298095703\n",
      "26.207014083862305\n",
      "25.740304946899414\n",
      "25.321687698364258\n",
      "24.9403018951416\n",
      "24.588842391967773\n",
      "24.262083053588867\n"
     ]
    }
   ],
   "source": [
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        defaults = {\"lr\": lr, \"weight_decay\" : 0.5}\n",
    "        print(type(params))\n",
    "        super().__init__(params, defaults)\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"] # Get the learning rate.\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p] # Get state associated with p.\n",
    "                t = state.get(\"t\", 0) # Get iteration number from the state, or initial value.\n",
    "                grad = p.grad.data # Get the gradient of loss with respect to p.\n",
    "                p.data -= lr / math.sqrt(t + 1) * grad # Update weight tensor in-place.\n",
    "                state[\"t\"] = t + 1 # Increment iteration number.\n",
    "        return loss\n",
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "opt = SGD([weights], lr=1)\n",
    "for t in range(10):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step() # Run optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4eada6cb-00b8-4944-beae-861385d46867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[  7.2185,  -2.4945,   0.3517,   2.3825,  -0.6931,  -1.1520,   2.5174,\n",
      "          -1.7059,  -3.5319,  -5.8812],\n",
      "        [  0.9454,   0.0879,  -4.3236,  12.6251,  -4.0296,  -1.3024,  -0.6779,\n",
      "          -3.5746,  14.3638,   3.5311],\n",
      "        [  3.1976,   8.9573,  -2.3486,   7.4501,  -2.0318,   1.1930,   0.3364,\n",
      "           3.4070,  -7.0912,   5.3901],\n",
      "        [ -2.4276,  -1.6256,  12.8364,  -2.2537,   3.0071,  -5.2473,  -4.8484,\n",
      "           7.3746,   5.5555,   2.7918],\n",
      "        [  5.7402,  -3.3578,   1.5090,   5.5736,   4.8676,  -2.5984,  -0.4501,\n",
      "          -5.5459,   1.5423, -10.2543],\n",
      "        [  8.4877,  -1.9380,  -2.1589,   6.4189,   0.1188,  -3.8334,  -2.8888,\n",
      "           5.0671,   7.1177,  -1.9052],\n",
      "        [ -6.0225,  -3.3425,   5.1899,  -4.8999,  -4.4232,  -0.2326,   2.7010,\n",
      "           4.6236,   1.8738,   9.0100],\n",
      "        [ -4.6705,   1.4479, -10.4200,   0.4330,  -2.1586,   3.3432,   4.7418,\n",
      "           3.8059,  -1.8448, -10.1266],\n",
      "        [  1.8026,  -3.3071,   2.7102,   7.9366,  -3.8372,  -2.1018,  -4.6966,\n",
      "           1.2223,  -1.6906,   3.2279],\n",
      "        [ -4.6910,   1.3243,  -2.5424,  -0.0335,  -6.2336,  -1.5919,   2.6876,\n",
      "          -3.9274,  -4.1138,   4.3763]], requires_grad=True): {'t': 10}})\n"
     ]
    }
   ],
   "source": [
    "print(opt.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a6e75be3-976a-4355-a0a6-1cf8de01aa93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': [Parameter containing:\n",
       "  tensor([[  7.2185,  -2.4945,   0.3517,   2.3825,  -0.6931,  -1.1520,   2.5174,\n",
       "            -1.7059,  -3.5319,  -5.8812],\n",
       "          [  0.9454,   0.0879,  -4.3236,  12.6251,  -4.0296,  -1.3024,  -0.6779,\n",
       "            -3.5746,  14.3638,   3.5311],\n",
       "          [  3.1976,   8.9573,  -2.3486,   7.4501,  -2.0318,   1.1930,   0.3364,\n",
       "             3.4070,  -7.0912,   5.3901],\n",
       "          [ -2.4276,  -1.6256,  12.8364,  -2.2537,   3.0071,  -5.2473,  -4.8484,\n",
       "             7.3746,   5.5555,   2.7918],\n",
       "          [  5.7402,  -3.3578,   1.5090,   5.5736,   4.8676,  -2.5984,  -0.4501,\n",
       "            -5.5459,   1.5423, -10.2543],\n",
       "          [  8.4877,  -1.9380,  -2.1589,   6.4189,   0.1188,  -3.8334,  -2.8888,\n",
       "             5.0671,   7.1177,  -1.9052],\n",
       "          [ -6.0225,  -3.3425,   5.1899,  -4.8999,  -4.4232,  -0.2326,   2.7010,\n",
       "             4.6236,   1.8738,   9.0100],\n",
       "          [ -4.6705,   1.4479, -10.4200,   0.4330,  -2.1586,   3.3432,   4.7418,\n",
       "             3.8059,  -1.8448, -10.1266],\n",
       "          [  1.8026,  -3.3071,   2.7102,   7.9366,  -3.8372,  -2.1018,  -4.6966,\n",
       "             1.2223,  -1.6906,   3.2279],\n",
       "          [ -4.6910,   1.3243,  -2.5424,  -0.0335,  -6.2336,  -1.5919,   2.6876,\n",
       "            -3.9274,  -4.1138,   4.3763]], requires_grad=True)],\n",
       " 'lr': 1,\n",
       " 'weight_decay': 0.5}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.param_groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c1569ebd-ad25-4ea9-b13b-f7dcadc34a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.091079711914062\n",
      "19.099563598632812\n",
      "13.836668968200684\n",
      "10.049921035766602\n",
      "7.290708065032959\n",
      "5.216639995574951\n",
      "3.6774072647094727\n",
      "2.5980215072631836\n",
      "1.8952690362930298\n",
      "1.4653306007385254\n"
     ]
    }
   ],
   "source": [
    "class AdamW(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas = (0.9, 0.999), weight_decay = 0.01, eps = 10e-8):\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        defaults = {\"lr\": lr, \n",
    "                    \"beta_1\" : betas[0], \n",
    "                    \"beta_2\" : betas[1],\n",
    "                   \"weight_decay\" : weight_decay,\n",
    "                   \"epsilon\" : eps}\n",
    "        super().__init__(params, defaults)\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"] # Get the learning rate.\n",
    "            beta_1 = group['beta_1']\n",
    "            beta_2 = group['beta_2']\n",
    "            weight_decay = group['weight_decay']\n",
    "            epsilon = group['epsilon']\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p] # Get state associated with p.\n",
    "                t = state.get(\"t\", 1)\n",
    "                m = state.get(\"firstMoment\", torch.zeros_like(p.data)) # Get iteration number from the state, or initial value.\n",
    "                v = state.get(\"secondMoment\", torch.zeros_like(p.data)) # Get iteration number from the state, or initial value.\n",
    "\n",
    "                grad = p.grad.data # Get the gradient of loss with respect to p.\n",
    "                m = beta_1 * m + (1 - beta_1) * grad\n",
    "                v = beta_2 * v + (1 - beta_2) * grad ** 2\n",
    "                adjustedLR = lr * ((1 - beta_2 ** t) ** 0.5) / (1 - beta_1 ** t)\n",
    "                p.data -= adjustedLR * m / (v ** 0.5 + epsilon)\n",
    "                p.data -= lr * weight_decay * p.data\n",
    "\n",
    "                state['t'] = t + 1\n",
    "                state['firstMoment'] = m\n",
    "                state['secondMoment'] = v\n",
    "                \n",
    "        return loss\n",
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "weights2 = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "\n",
    "opt = AdamW([weights, weights2], lr=1)\n",
    "for t in range(10):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step() # Run optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "efb80b75-454e-48eb-a93f-8b350cffad98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.787813186645508\n",
      "defaultdict(<class 'dict'>, {})\n"
     ]
    }
   ],
   "source": [
    "def lrScheduler(t, alphaMax, alphaMin, Tw, Tc):\n",
    "    if t < Tw:\n",
    "        return t / Tw * alphaMax\n",
    "    elif t <= Tc:\n",
    "        cosPortion = math.cos((t - Tw) / (Tc - Tw) * math.pi)\n",
    "        return alphaMin + 0.5 * (1 + cosPortion) * (alphaMax - alphaMin)\n",
    "    else:\n",
    "        return alphaMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402ccf9-e6c4-4982-a605-7ed77aadb930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientClipper(paramList, maxNorm, eps=1e-6):\n",
    "    norm = 0\n",
    "    for param in paramList:\n",
    "        if param.grad is None:\n",
    "            continue\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "        \n",
    "    norm = norm ** 0.5\n",
    "    \n",
    "    if norm < maxNorm:\n",
    "        return\n",
    "        \n",
    "    for param in paramList:\n",
    "        param.grad.data *= maxNorm / (norm + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb76ada7-658c-4952-98ba-ebdf53f343e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
