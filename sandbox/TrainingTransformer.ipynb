{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8706c703-d8ae-4438-878e-bf514465e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67380070-1dfd-42b5-af76-4a2c347dc6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1587, 0.0488, 0.4874, 0.2383, 0.0668],\n",
       "        [0.2220, 0.1276, 0.0781, 0.2395, 0.3328]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from cs336_basics.transformerStuff import softmax\n",
    "\n",
    "exampleLogits = torch.randn(2, 5)\n",
    "exampleIndices = torch.tensor([0, 2])\n",
    "\n",
    "probs = softmax(exampleLogits, dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cb4cc05-a255-4123-808e-4bdf5f2ecb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einx\n",
    "trueLogits = einx.get_at(\"b [v], (b [idx]) -> b 1\", exampleLogits, exampleIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62c2f985-a1bc-452a-8524-defbbe6b3278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7186],\n",
       "        [1.1003]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eLogits = torch.exp(exampleLogits - exampleLogits.max(dim=-1, keepdim=True).values)\n",
    "logSumExp = torch.log(eLogits.sum(axis=-1, keepdim=True))\n",
    "logSumExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee609f1d-b453-4c00-8a63-637539717511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2175)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(- (trueLogits - logSumExp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "469c48f4-8a8b-425d-9fab-e1aebc3232ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(inputs, targets):\n",
    "    \"\"\"\n",
    "        Given a tensor of inputs and targets, compute the average cross-entropy\n",
    "        loss across examples.\n",
    "    \n",
    "        Args:\n",
    "            inputs (Float[Tensor, \"batch_size vocab_size\"]): inputs[i][j] is the\n",
    "                unnormalized logit of jth class for the ith example.\n",
    "            targets (Int[Tensor, \"batch_size\"]): Tensor of shape (batch_size,) with the index of the correct class.\n",
    "                Each value must be between 0 and `num_classes - 1`.\n",
    "    \n",
    "        Returns:\n",
    "            Float[Tensor, \"\"]: The average cross-entropy loss across examples.\n",
    "    \"\"\"\n",
    "    inputs = einx.rearrange(\"... v -> (...) v\", inputs)\n",
    "    targets = einx.rearrange(\"... -> (...)\", targets)\n",
    "    inputs = inputs - inputs.max(dim=-1, keepdim=True).values\n",
    "    trueLogits = einx.get_at(\"b [v], (b [idx]) -> b 1\", inputs, targets)\n",
    "    eLogits = torch.exp(inputs)\n",
    "    logSumExp = torch.log(eLogits.sum(axis=-1, keepdim=True))\n",
    "    return torch.mean(- (trueLogits - logSumExp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dcdf1122-28e7-4acc-bb3e-9369cbc157c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1950)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(exampleLogits, exampleIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0fe8d2e7-7358-4b17-b0c0-a2664bdeac96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6095)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs =  torch.tensor([[0.1088, 0.1060, 0.6683, 0.5131, 0.0645],\n",
    "               [0.4538, 0.6852, 0.2520, 0.3792, 0.2675],\n",
    "               [0.4578, 0.3357, 0.6384, 0.0481, 0.5612],\n",
    "               [0.9639, 0.8864, 0.1585, 0.3038, 0.0350],\n",
    "               [0.3356, 0.9013, 0.7052, 0.8294, 0.8334],\n",
    "              [0.6333, 0.4434, 0.1428, 0.5739, 0.3810],\n",
    "              [0.9476, 0.5917, 0.7037, 0.2987, 0.6208],\n",
    "               [0.8541, 0.1803, 0.2054, 0.4775, 0.8199]])\n",
    "targets = torch.tensor([1, 0, 2, 2, 4, 1, 4, 0])\n",
    "cross_entropy(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b302f32-2881-4bcf-8605-11f8a41b9470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0685)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.randn(3, 5, 7)\n",
    "targets = torch.randint(0, 7, size=(3, 5))\n",
    "cross_entropy(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa3495dd-672e-4ff8-918d-a57e72acbccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Iterable\n",
    "from typing import Optional\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d5d2a8e6-1ca1-472c-9b0c-e41e5236b2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "26.3189640045166\n",
      "25.2767333984375\n",
      "24.566858291625977\n",
      "24.002788543701172\n",
      "23.525131225585938\n",
      "23.106184005737305\n",
      "22.73040008544922\n",
      "22.388046264648438\n",
      "22.072553634643555\n",
      "21.77923011779785\n"
     ]
    }
   ],
   "source": [
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        defaults = {\"lr\": lr}\n",
    "        print(type(params))\n",
    "        super().__init__(params, defaults)\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"] # Get the learning rate.\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p] # Get state associated with p.\n",
    "                t = state.get(\"t\", 0) # Get iteration number from the state, or initial value.\n",
    "                grad = p.grad.data # Get the gradient of loss with respect to p.\n",
    "                p.data -= lr / math.sqrt(t + 1) * grad # Update weight tensor in-place.\n",
    "                state[\"t\"] = t + 1 # Increment iteration number.\n",
    "        return loss\n",
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "opt = SGD([weights], lr=1)\n",
    "for t in range(10):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step() # Run optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4eada6cb-00b8-4944-beae-861385d46867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {})\n"
     ]
    }
   ],
   "source": [
    "print(opt.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a6e75be3-976a-4355-a0a6-1cf8de01aa93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[ -0.0617,  -5.3531,  -0.1866,  -2.1943,  -6.7153,  -4.1213,   4.0880,\n",
       "              4.8780,   0.1285,   1.5604],\n",
       "           [ -9.5234,   6.2102,  -4.3096,   3.5231,   2.8335,   3.1488,   3.9583,\n",
       "              9.0442,  -0.5850,   3.7255],\n",
       "           [ -0.6307,  -0.3854,  -7.6866,   0.1314,  -1.0857,   2.3122,  -8.5276,\n",
       "             -2.1388,   5.3930,  -0.6250],\n",
       "           [  8.4559,   2.1054,   6.6357,  -0.4007,  -1.8352,   1.6471,   4.2291,\n",
       "              0.7674,  -1.5360,   4.4736],\n",
       "           [ -3.4156,  -4.5099,   0.9973,   2.2794,   1.7119,   5.8206,   2.9674,\n",
       "             -2.1727,  -1.0410,  -0.6734],\n",
       "           [  4.3551,   5.6291,   1.9736,  -4.2587,  -5.4546,  -8.3573,  -8.0197,\n",
       "             -0.7031,   1.0147,   1.3910],\n",
       "           [  9.5710,  -9.4592,   0.4313,  -5.2994,  -1.6013,  -5.0713,  -0.9703,\n",
       "              3.6844,   6.7415,   7.0759],\n",
       "           [  0.5346,   6.6267,   2.0775,   1.7978,   2.5313,  -5.4249,  -2.2080,\n",
       "             -6.0796,   4.8889,  -1.2524],\n",
       "           [ 10.3816,  -3.8224,  10.9891,   3.7586,  -0.2893,  -4.9622,   0.1195,\n",
       "              0.1194,  -2.1984,   3.4245],\n",
       "           [ -2.8670,   3.6624,   8.0328,   0.6357,  -1.4853,  -5.1772,  -2.1478,\n",
       "             -0.8239,   2.9120, -13.1446]], requires_grad=True)],\n",
       "  'lr': 1}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c1569ebd-ad25-4ea9-b13b-f7dcadc34a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.091079711914062\n",
      "19.099563598632812\n",
      "13.836668968200684\n",
      "10.049921035766602\n",
      "7.290708065032959\n",
      "5.216639995574951\n",
      "3.6774072647094727\n",
      "2.5980215072631836\n",
      "1.8952690362930298\n",
      "1.4653306007385254\n"
     ]
    }
   ],
   "source": [
    "class AdamW(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas = (0.9, 0.999), weight_decay = 0.01, eps = 10e-8):\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        defaults = {\"lr\": lr, \n",
    "                    \"beta_1\" : betas[0], \n",
    "                    \"beta_2\" : betas[1],\n",
    "                   \"weight_decay\" : weight_decay,\n",
    "                   \"epsilon\" : eps}\n",
    "        super().__init__(params, defaults)\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"] # Get the learning rate.\n",
    "            beta_1 = group['beta_1']\n",
    "            beta_2 = group['beta_2']\n",
    "            weight_decay = group['weight_decay']\n",
    "            epsilon = group['epsilon']\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p] # Get state associated with p.\n",
    "                t = state.get(\"t\", 1)\n",
    "                m = state.get(\"firstMoment\", torch.zeros_like(p.data)) # Get iteration number from the state, or initial value.\n",
    "                v = state.get(\"secondMoment\", torch.zeros_like(p.data)) # Get iteration number from the state, or initial value.\n",
    "\n",
    "                grad = p.grad.data # Get the gradient of loss with respect to p.\n",
    "                m = beta_1 * m + (1 - beta_1) * grad\n",
    "                v = beta_2 * v + (1 - beta_2) * grad ** 2\n",
    "                adjustedLR = lr * ((1 - beta_2 ** t) ** 0.5) / (1 - beta_1 ** t)\n",
    "                p.data -= adjustedLR * m / (v ** 0.5 + epsilon)\n",
    "                p.data -= lr * weight_decay * p.data\n",
    "\n",
    "                state['t'] = t + 1\n",
    "                state['firstMoment'] = m\n",
    "                state['secondMoment'] = v\n",
    "                \n",
    "        return loss\n",
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "weights2 = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "\n",
    "opt = AdamW([weights, weights2], lr=1)\n",
    "for t in range(10):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step() # Run optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "efb80b75-454e-48eb-a93f-8b350cffad98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.787813186645508\n",
      "defaultdict(<class 'dict'>, {})\n"
     ]
    }
   ],
   "source": [
    "def lrScheduler(t, alphaMax, alphaMin, Tw, Tc):\n",
    "    if t < Tw:\n",
    "        return t / Tw * alphaMax\n",
    "    elif t <= Tc:\n",
    "        cosPortion = math.cos((t - Tw) / (Tc - Tw) * math.pi)\n",
    "        return alphaMin + 0.5 * (1 + cosPortion) * (alphaMax - alphaMin)\n",
    "    else:\n",
    "        return alphaMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402ccf9-e6c4-4982-a605-7ed77aadb930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
