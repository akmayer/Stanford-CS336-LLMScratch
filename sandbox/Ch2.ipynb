{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b874dc9-08bf-47be-a618-b548a3b5066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einx\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        w = torch.empty(out_features, in_features)\n",
    "        std = (2 / (in_features + out_features)) ** 1/2\n",
    "        nn.init.trunc_normal_(w, mean = 0, std = std, a = -3 * std, b = 3 * std)\n",
    "        self.mat = nn.Parameter(w)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        return einx.dot(\"out [in], ... [in] -> ... out\", self.mat, x)\n",
    "\n",
    "class MyEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        w = torch.empty(num_embeddings, embedding_dim)\n",
    "        nn.init.trunc_normal_(w, mean = 0, std = 1, a = -3 , b = 3)\n",
    "        self.mat = nn.Parameter(w)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        return self.mat[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c070e75e-81fb-4763-93c7-741b1a197bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1431, -0.4317, -0.6513, -0.2015,  1.3787, -0.9340,  0.6720],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.empty(3, 7)\n",
    "nn.init.trunc_normal_(w, mean = 0, std = 1, a = -3, b = 3)\n",
    "x = nn.Parameter(w)\n",
    "\n",
    "x[torch.LongTensor([[1, 2, 2, 2], [0, 0, 0, 0]])][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b841693-15be-4f0c-bee5-562c9188428d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 0.])\n",
      "tensor([2., 0.])\n",
      "tensor([1.4142, 0.0032])\n",
      "tensor([[1.4142, 0.7071, 0.7071],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.4142, 0.7071, 0.7071],\n",
       "        [0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyRMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        gain = torch.ones(d_model)\n",
    "        self.gain = nn.Parameter(gain)\n",
    "        self.d_model = d_model\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        in_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        RMS_A = x ** 2\n",
    "        RMS_A = RMS_A.sum(axis=-1)\n",
    "        print(RMS_A)\n",
    "        RMS_A /= self.d_model\n",
    "        print(RMS_A)\n",
    "        RMS_A += self.eps\n",
    "        RMS_A = RMS_A ** (1/2)\n",
    "        print(RMS_A)\n",
    "        RMS_A = RMS_A.unsqueeze(-1)\n",
    "        x = x / RMS_A * self.gain\n",
    "        print(x)\n",
    "        return x.to(in_dtype)\n",
    "rms = MyRMSNorm(3)\n",
    "\n",
    "batch = torch.tensor([[2, 1, 1.], [0, 0, 0]])\n",
    "rms(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc079fe-934b-40ed-83b8-836a8583cc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0899e242-4c56-4b8f-b04d-be64f120988d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.         -0.        ]\n",
      "  [ 0.          1.        ]\n",
      "  [ 1.         -0.        ]\n",
      "  [ 0.          1.        ]]\n",
      "\n",
      " [[ 0.54030231 -0.84147098]\n",
      "  [ 0.84147098  0.54030231]\n",
      "  [ 0.7602446  -0.64963694]\n",
      "  [ 0.64963694  0.7602446 ]]]\n",
      "tensor([[0, 0, 2, 2]])\n",
      "tensor([[0.5403, 0.8415, 0.7602, 0.6496]], dtype=torch.float64)\n",
      "\n",
      "tensor([[1, 1, 3, 3]])\n",
      "tensor([[-0.8415,  0.5403, -0.6496,  0.7602]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8415,  0.5403, -0.4284,  3.5800]], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class MyRope(nn.Module):\n",
    "    def __init__(self, d_key, theta, max_seq_length, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        seqOfThetaArr = []\n",
    "        for i in range(max_seq_length):\n",
    "            thetaArr = []\n",
    "            k = 1\n",
    "            for idx in range(d_key):\n",
    "                tik = i / (theta ** ((2 * k - 2)/d_key))\n",
    "                if idx % 2 == 0:\n",
    "                    thetaArr.append(np.array([np.cos(tik), -np.sin(tik)]))\n",
    "                else:\n",
    "                    thetaArr.append(np.array([np.sin(tik), np.cos(tik)]))\n",
    "                    k += 1\n",
    "            seqOfThetaArr.append(np.stack(thetaArr))\n",
    "        precompThetaArr = np.stack(seqOfThetaArr)\n",
    "        print(precompThetaArr)\n",
    "        self.register_buffer(\"rotaryTable\", torch.tensor(precompThetaArr), persistent=False)\n",
    "        self.d_key = d_key\n",
    "        self.evenIndices = [x if x % 2 == 0 else x - 1 for x in range(d_key)]\n",
    "        self.oddIndices = [x if x % 2 == 1 else x + 1 for x in range(d_key)]\n",
    "\n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n",
    "        token_positions = token_positions.unsqueeze(-1)\n",
    "        tablesOfInterest = einx.get_at(\"[i] k z, ... [idx] -> ... k z\", self.rotaryTable, token_positions)\n",
    "        evenX = x[..., self.evenIndices]\n",
    "        oddX = x[..., self.oddIndices]\n",
    "        evenTables = tablesOfInterest[..., 0]\n",
    "        oddTables = tablesOfInterest[..., 1]\n",
    "\n",
    "        print(evenX)\n",
    "        print(evenTables)\n",
    "        print()\n",
    "        print(oddX)\n",
    "        print(oddTables)\n",
    "\n",
    "        return evenX * evenTables + oddX * oddTables\n",
    "rope = MyRope(4, 2, 2)\n",
    "rope(torch.tensor([[0, 1, 2, 3]]), torch.tensor([1,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aded822c-7562-4af9-a9db-876af8fdb33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0066, 0.0179, 0.9756],\n",
       "        [0.1173, 0.8668, 0.0159]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x: torch.Tensor, dim):\n",
    "    x = x - x.max(dim=dim, keepdim=True).values\n",
    "    denom = torch.sum(torch.exp(x), dim=dim, keepdim=True)\n",
    "    x = torch.exp(x) / denom\n",
    "    return x\n",
    "\n",
    "softmax(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f711fb4e-9c63-4f0e-84ca-e5bac7e31d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1192, 0.0474, 0.9933],\n",
       "        [0.8808, 0.9526, 0.0067]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [0, 1, 5],\n",
    "    [2, 4, 0]\n",
    "])\n",
    "\n",
    "z = x - x.max(dim=0, keepdim=True).values\n",
    "\n",
    "denom = torch.sum(torch.exp(z), dim=0, keepdim=True)\n",
    "torch.exp(z) / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f41e474-ec00-4b4f-b30a-10693fa8cf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0059, 0.0174, 0.9933],\n",
       "        [0.1192, 0.9526, 0.0182]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x - x.max(dim=1, keepdim=True).values\n",
    "torch.sum(torch.exp(z), dim=1, keepdim=True)\n",
    "torch.exp(z) / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b446462e-a33b-4398-acc4-2b9213228b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = torch.randn(2, 3, 5)\n",
    "queries = torch.randn(2, 1, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd1acf1b-3928-42b4-ba9f-ba110b5e3a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2156, 0.7844, 0.0000]],\n",
       "\n",
       "        [[0.6810, 0.3190, 0.0000]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.tensor([\n",
    "    [\n",
    "        [5., 5],\n",
    "        [-1, -1],\n",
    "        [-1, -1]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0],\n",
    "        [1, 1],\n",
    "        [2, 2]\n",
    "    ]\n",
    "])\n",
    "\n",
    "mask = torch.tensor([[True, True, False]])\n",
    "\n",
    "def scaledDotProdAttention(queries, keys, values, mask=None):\n",
    "\n",
    "    presoftAttention = einx.dot(\"b ... key [dim], b ... quer [dim] -> b ... quer key\", keys, queries) / keys.shape[-1] ** 0.5\n",
    "    if mask is not None:\n",
    "        presoftAttention[~mask.expand(presoftAttention.shape)] = - torch.inf\n",
    "    \n",
    "    softAttention = softmax(presoftAttention, dim = -1)\n",
    "    return einx.dot(\"b ... quer [key], b ... [key] d_v -> b ... quer d_v\", softAttention, values)\n",
    "softAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "978dc89d-ae68-4b5a-85bb-fc4d8e2e6cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2933, 0.2933]],\n",
       "\n",
       "        [[0.3190, 0.3190]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11867803-4d85-4b2e-9b9d-2d0bb0848aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([102., 100.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.tensor([0.5, 1, 0.5])\n",
    "vals = torch.tensor([\n",
    "    [1, 0.],\n",
    "    [100, 100],\n",
    "    [3, 0]\n",
    "])\n",
    "einx.dot(\"[s], [s] k -> k\", z, vals, a=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d6b972-2512-4007-bc94-3d08cb596c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
