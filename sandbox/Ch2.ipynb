{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b874dc9-08bf-47be-a618-b548a3b5066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einx\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        w = torch.empty(out_features, in_features)\n",
    "        std = (2 / (in_features + out_features)) ** 1/2\n",
    "        nn.init.trunc_normal_(w, mean = 0, std = std, a = -3 * std, b = 3 * std)\n",
    "        self.mat = nn.Parameter(w)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        return einx.dot(\"out [in], ... [in] -> ... out\", self.mat, x)\n",
    "\n",
    "class MyEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        w = torch.empty(num_embeddings, embedding_dim)\n",
    "        nn.init.trunc_normal_(w, mean = 0, std = 1, a = -3 , b = 3)\n",
    "        self.mat = nn.Parameter(w)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        return self.mat[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c070e75e-81fb-4763-93c7-741b1a197bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1431, -0.4317, -0.6513, -0.2015,  1.3787, -0.9340,  0.6720],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.empty(3, 7)\n",
    "nn.init.trunc_normal_(w, mean = 0, std = 1, a = -3, b = 3)\n",
    "x = nn.Parameter(w)\n",
    "\n",
    "x[torch.LongTensor([[1, 2, 2, 2], [0, 0, 0, 0]])][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b841693-15be-4f0c-bee5-562c9188428d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 0.])\n",
      "tensor([2., 0.])\n",
      "tensor([1.4142, 0.0032])\n",
      "tensor([[1.4142, 0.7071, 0.7071],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.4142, 0.7071, 0.7071],\n",
       "        [0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyRMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        gain = torch.ones(d_model)\n",
    "        self.gain = nn.Parameter(gain)\n",
    "        self.d_model = d_model\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        in_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        RMS_A = x ** 2\n",
    "        RMS_A = RMS_A.sum(axis=-1)\n",
    "        print(RMS_A)\n",
    "        RMS_A /= self.d_model\n",
    "        print(RMS_A)\n",
    "        RMS_A += self.eps\n",
    "        RMS_A = RMS_A ** (1/2)\n",
    "        print(RMS_A)\n",
    "        RMS_A = RMS_A.unsqueeze(-1)\n",
    "        x = x / RMS_A * self.gain\n",
    "        print(x)\n",
    "        return x.to(in_dtype)\n",
    "rms = MyRMSNorm(3)\n",
    "\n",
    "batch = torch.tensor([[2, 1, 1.], [0, 0, 0]])\n",
    "rms(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc079fe-934b-40ed-83b8-836a8583cc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0899e242-4c56-4b8f-b04d-be64f120988d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 3]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class MyRope(nn.Module):\n",
    "    def __init__(self, d_key, theta, max_seq_length, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        seqOfThetaArr = []\n",
    "        for i in range(max_seq_length):\n",
    "            thetaArr = []\n",
    "            k = 1\n",
    "            for idx in range(d_key):\n",
    "                tik = i / (theta ** ((2 * k - 2)/d_key) )\n",
    "                if idx % 2 == 0:\n",
    "                    thetaArr.append(np.array([np.cos(tik), -np.sin(tik)]))\n",
    "                else:\n",
    "                    thetaArr.append(np.array([np.sin(tik), np.cos(tik)]))\n",
    "                    k += 1\n",
    "            seqOfThetaArr.append(np.stack(thetaArr))\n",
    "        precompThetaArr = np.stack(seqOfThetaArr)\n",
    "        self.register_buffer(\"rotaryTable\", torch.tensor(precompThetaArr), persistent=False)\n",
    "        self.d_key = d_key\n",
    "        self.evenIndices = [x if x % 2 == 0 else x - 1 for x in range(d_key)]\n",
    "        self.oddIndices = [x if x % 2 == 1 else x + 1 for x in range(d_key)]\n",
    "\n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n",
    "        token_positions = token_positions.unsqueeze(-1)\n",
    "        tablesOfInterest = einx.get_at(\"[i] k z, ... [idx] -> ... k z\", self.rotaryTable, token_positions)\n",
    "        evenX = x[..., self.evenIndices]\n",
    "        oddX = x[..., self.oddIndices]\n",
    "        evenTables = tablesOfInterest[..., 0]\n",
    "        oddTables = tablesOfInterest[..., 1]\n",
    "\n",
    "        return (evenX * evenTables + oddX * oddTables).to(x.dtype)\n",
    "rope = MyRope(4, 2, 2)\n",
    "rope(torch.tensor([[0, 1, 2, 3]]), torch.tensor([1,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aded822c-7562-4af9-a9db-876af8fdb33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0066, 0.0179, 0.9756],\n",
       "        [0.1173, 0.8668, 0.0159]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x: torch.Tensor, dim):\n",
    "    x = x - x.max(dim=dim, keepdim=True).values\n",
    "    denom = torch.sum(torch.exp(x), dim=dim, keepdim=True)\n",
    "    x = torch.exp(x) / denom\n",
    "    return x\n",
    "\n",
    "softmax(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f711fb4e-9c63-4f0e-84ca-e5bac7e31d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1192, 0.0474, 0.9933],\n",
       "        [0.8808, 0.9526, 0.0067]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [0, 1, 5],\n",
    "    [2, 4, 0]\n",
    "])\n",
    "\n",
    "z = x - x.max(dim=0, keepdim=True).values\n",
    "\n",
    "denom = torch.sum(torch.exp(z), dim=0, keepdim=True)\n",
    "torch.exp(z) / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f41e474-ec00-4b4f-b30a-10693fa8cf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0059, 0.0174, 0.9933],\n",
       "        [0.1192, 0.9526, 0.0182]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x - x.max(dim=1, keepdim=True).values\n",
    "torch.sum(torch.exp(z), dim=1, keepdim=True)\n",
    "torch.exp(z) / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b446462e-a33b-4398-acc4-2b9213228b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = torch.randn(2, 3, 5)\n",
    "queries = torch.randn(2, 1, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fd1acf1b-3928-42b4-ba9f-ba110b5e3a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2156, 0.7844, 0.0000]],\n",
       "\n",
       "        [[0.6810, 0.3190, 0.0000]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.tensor([\n",
    "    [\n",
    "        [5., 5],\n",
    "        [-1, -1],\n",
    "        [-1, -1]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0],\n",
    "        [1, 1],\n",
    "        [2, 2]\n",
    "    ]\n",
    "])\n",
    "\n",
    "mask = torch.tensor([[True, True, False]])\n",
    "\n",
    "def scaledDotProdAttention(queries, keys, values, mask=None):\n",
    "\n",
    "    presoftAttention = einx.dot(\"b ... key [dim], b ... quer [dim] -> b ... quer key\", keys, queries) / keys.shape[-1] ** 0.5\n",
    "    if mask is not None:\n",
    "        presoftAttention[~mask.expand(presoftAttention.shape)] = - torch.inf\n",
    "    \n",
    "    softAttention = softmax(presoftAttention, dim = -1)\n",
    "    return einx.dot(\"b ... quer [key], b ... [key] d_v -> b ... quer d_v\", softAttention, values)\n",
    "softAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "978dc89d-ae68-4b5a-85bb-fc4d8e2e6cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2933, 0.2933]],\n",
       "\n",
       "        [[0.3190, 0.3190]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einx.dot(\"b ... quer [key], b ... [key] d_v -> b ... quer d_v\", softAttention, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "11867803-4d85-4b2e-9b9d-2d0bb0848aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([102., 100.])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fakeAttention = torch.tensor([0.5, 1, 0.5])\n",
    "vals = torch.tensor([\n",
    "    [1, 0.],\n",
    "    [100, 100],\n",
    "    [3, 0]\n",
    "])\n",
    "einx.dot(\"[s], [s] k -> k\", fakeAttention, vals, a=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f7d6b972-2512-4007-bc94-3d08cb596c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.6356e-03,  2.7596e-03,  1.3597e-02,  1.7189e-02,  1.4190e-02,\n",
       "           6.9568e-03,  3.5568e-02,  3.4001e-03],\n",
       "         [ 2.8591e-02,  4.3619e-03,  2.5728e-02, -9.0700e-03,  1.2817e-03,\n",
       "           1.8614e-03,  3.1938e-02, -4.8912e-03],\n",
       "         [-1.6222e-03,  2.9483e-03,  3.3452e-04,  7.3399e-03,  4.7355e-03,\n",
       "          -3.2652e-05,  9.7079e-03, -6.0467e-03],\n",
       "         [-9.9415e-03,  4.8823e-03, -3.5935e-03,  9.5569e-03,  5.3491e-03,\n",
       "           3.8651e-03,  8.7364e-03, -2.3996e-05],\n",
       "         [-7.2212e-03,  7.8951e-03, -1.1571e-05,  7.1348e-03,  2.5053e-03,\n",
       "           1.6936e-03,  1.1393e-02,  5.0010e-03]],\n",
       "\n",
       "        [[ 5.6048e-02, -4.6875e-02,  6.7406e-02, -1.8357e-02,  5.8747e-03,\n",
       "          -4.3492e-02,  1.8585e-02, -2.8123e-02],\n",
       "         [ 1.4272e-02, -3.6911e-02,  2.9286e-02, -1.7568e-03,  6.8680e-03,\n",
       "          -2.8475e-02, -5.3976e-03, -1.4381e-02],\n",
       "         [ 5.8715e-03, -2.1917e-02,  1.2103e-02, -3.2530e-03,  5.3082e-03,\n",
       "          -1.2299e-02, -9.1095e-03, -1.4640e-02],\n",
       "         [ 6.9578e-03, -1.2334e-02,  1.7501e-02, -1.2740e-03,  2.6334e-03,\n",
       "          -1.0121e-02,  5.4493e-03, -3.3791e-04],\n",
       "         [ 8.9607e-03, -9.6271e-03,  1.3976e-02, -2.2431e-03,  1.6234e-03,\n",
       "          -6.0087e-03,  4.8699e-03, -1.4318e-04]],\n",
       "\n",
       "        [[-5.0709e-03,  4.0230e-03, -2.7109e-03,  4.0913e-04, -1.2641e-03,\n",
       "          -1.2056e-02, -8.4336e-03, -6.6980e-03],\n",
       "         [-1.1151e-02, -4.4351e-03,  4.0347e-03, -7.9031e-04,  4.1514e-03,\n",
       "          -1.4041e-02, -1.1115e-02, -5.3236e-03],\n",
       "         [ 9.0326e-03,  2.0512e-03,  8.6749e-03, -1.1287e-02, -4.0835e-03,\n",
       "          -3.5035e-03,  7.6093e-04,  9.7272e-04],\n",
       "         [ 9.2061e-03,  1.3974e-02,  7.1599e-03, -6.7898e-03, -4.6704e-03,\n",
       "           3.2052e-03,  1.2720e-02,  1.0341e-02],\n",
       "         [ 2.0748e-02,  9.8895e-03,  1.9305e-02, -1.1715e-02, -4.4304e-03,\n",
       "          -6.0210e-04,  1.9934e-02,  6.2671e-03]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dk = d_model // num_heads\n",
    "\n",
    "        self.Wq = MyLinear(d_model, d_model)\n",
    "        self.Wk = MyLinear(d_model, d_model)\n",
    "        self.Wv = MyLinear(d_model, d_model)\n",
    "        self.Wo = MyLinear(d_model, d_model)\n",
    "    def forward(self, x):\n",
    "        seq = x.shape[-2]\n",
    "        queries = self.Wq(x)\n",
    "        keys = self.Wk(x)\n",
    "        values = self.Wv(x)\n",
    "        queries = einx.rearrange(\"... seq (heads dk) -> ... heads seq dk\", queries, dk=self.dk)\n",
    "        keys = einx.rearrange(\"... seq (heads dk) -> ... heads seq dk\", keys, dk=self.dk)\n",
    "        values = einx.rearrange(\"... seq (heads dk) -> ... heads seq dk\", values, dk=self.dk)\n",
    "        mask = torch.tril(torch.ones((seq, seq))).bool()\n",
    "        attended = scaledDotProdAttention(queries, keys, values, mask)\n",
    "        multiHead = einx.rearrange(\"... heads seq dk -> ... seq (heads dk)\", attended)\n",
    "        return self.Wo(multiHead)\n",
    "\n",
    "mhsa = MultiHeadSelfAttention(8, 2)\n",
    "mhsa(torch.randn(3, 5, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "371b5b07-3c32-48a0-8916-c2d1defdef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0183,  0.0232, -0.0330,  0.0124,  0.0167, -0.0213, -0.0121,\n",
       "           0.0343],\n",
       "         [-0.0160,  0.0069, -0.0132, -0.0006, -0.0093, -0.0103,  0.0054,\n",
       "           0.0082],\n",
       "         [ 0.0025,  0.0078,  0.0001, -0.0079, -0.0146, -0.0042,  0.0045,\n",
       "           0.0096],\n",
       "         [-0.0040, -0.0073,  0.0083, -0.0046, -0.0153,  0.0078,  0.0082,\n",
       "          -0.0047],\n",
       "         [-0.0104, -0.0101,  0.0114, -0.0054, -0.0137,  0.0085,  0.0082,\n",
       "          -0.0050]],\n",
       "\n",
       "        [[ 0.0026,  0.0199, -0.0410,  0.0478,  0.0391, -0.0033, -0.0245,\n",
       "           0.0200],\n",
       "         [-0.0072,  0.0125, -0.0237,  0.0472,  0.0413, -0.0006, -0.0246,\n",
       "           0.0041],\n",
       "         [ 0.0085,  0.0062, -0.0120,  0.0297,  0.0246,  0.0077, -0.0151,\n",
       "           0.0072],\n",
       "         [ 0.0037,  0.0073, -0.0027,  0.0286,  0.0210,  0.0057, -0.0162,\n",
       "          -0.0012],\n",
       "         [ 0.0022,  0.0048, -0.0017,  0.0251,  0.0187,  0.0066, -0.0181,\n",
       "          -0.0027]],\n",
       "\n",
       "        [[ 0.0132, -0.0215,  0.0254, -0.0368, -0.0446,  0.0126,  0.0215,\n",
       "          -0.0258],\n",
       "         [ 0.0110, -0.0068,  0.0197, -0.0257, -0.0209,  0.0054, -0.0027,\n",
       "          -0.0068],\n",
       "         [-0.0016, -0.0059,  0.0043, -0.0092, -0.0137,  0.0041, -0.0042,\n",
       "          -0.0084],\n",
       "         [ 0.0074,  0.0047, -0.0038,  0.0060, -0.0024,  0.0036, -0.0015,\n",
       "           0.0073],\n",
       "         [ 0.0097, -0.0003,  0.0004, -0.0005, -0.0003,  0.0059, -0.0056,\n",
       "           0.0051]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadSelfAttentionRope(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_seq_len, theta, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dk = d_model // num_heads\n",
    "\n",
    "        self.rope = MyRope(self.dk, theta, max_seq_len)\n",
    "\n",
    "        self.Wq = MyLinear(d_model, d_model)\n",
    "        self.Wk = MyLinear(d_model, d_model)\n",
    "        self.Wv = MyLinear(d_model, d_model)\n",
    "        self.Wo = MyLinear(d_model, d_model)\n",
    "    def forward(self, x, token_positions):\n",
    "        seq = x.shape[-2]\n",
    "        queries = self.Wq(x)\n",
    "        keys = self.Wk(x)\n",
    "        values = self.Wv(x)\n",
    "        queries = einx.rearrange(\"... seq (heads dk) -> ... heads seq dk\", queries, dk=self.dk)\n",
    "        keys = einx.rearrange(\"... seq (heads dk) -> ... heads seq dk\", keys, dk=self.dk)\n",
    "        values = einx.rearrange(\"... seq (heads dk) -> ... heads seq dk\", values, dk=self.dk)\n",
    "\n",
    "        queries = self.rope(queries, token_positions.unsqueeze(-2).expand(keys.shape[:-1]))\n",
    "        keys = self.rope(keys, token_positions.unsqueeze(-2).expand(keys.shape[:-1]))\n",
    "        \n",
    "        mask = torch.tril(torch.ones((seq, seq))).bool()\n",
    "\n",
    "        print(queries.dtype)\n",
    "        attended = scaledDotProdAttention(queries, keys, values, mask)\n",
    "        multiHead = einx.rearrange(\"... heads seq dk -> ... seq (heads dk)\", attended)\n",
    "        return self.Wo(multiHead)\n",
    "mhsar = MultiHeadSelfAttentionRope(8, 2, 5, 1e-5)\n",
    "mhsar(torch.randn(3, 5, 8), torch.arange(5).expand(torch.randn(3, 5, 8).shape[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "281b3567-df1b-480e-bdb5-e19f88ae6db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.4330, -0.8483],\n",
       "          [-1.3166,  0.3196],\n",
       "          [ 0.4914,  0.1645],\n",
       "          [ 0.1627, -0.5318],\n",
       "          [-2.2035, -0.7041]],\n",
       "\n",
       "         [[ 1.3143, -1.1297],\n",
       "          [-1.7467, -0.7517],\n",
       "          [-0.7964,  0.3968],\n",
       "          [ 1.1945, -1.2584],\n",
       "          [ 0.0286,  0.8122]],\n",
       "\n",
       "         [[ 1.4008, -0.3317],\n",
       "          [-0.2632, -1.4176],\n",
       "          [-1.3153,  1.2273],\n",
       "          [ 0.1409,  1.1128],\n",
       "          [ 0.3915, -1.7735]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4330, -0.8483],\n",
       "          [-1.3166,  0.3196],\n",
       "          [ 0.4914,  0.1645],\n",
       "          [ 0.1627, -0.5318],\n",
       "          [-2.2035, -0.7041]],\n",
       "\n",
       "         [[ 1.3143, -1.1297],\n",
       "          [-1.7467, -0.7517],\n",
       "          [-0.7964,  0.3968],\n",
       "          [ 1.1945, -1.2584],\n",
       "          [ 0.0286,  0.8122]],\n",
       "\n",
       "         [[ 1.4008, -0.3317],\n",
       "          [-0.2632, -1.4176],\n",
       "          [-1.3153,  1.2273],\n",
       "          [ 0.1409,  1.1128],\n",
       "          [ 0.3915, -1.7735]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4330, -0.8483],\n",
       "          [-1.3166,  0.3196],\n",
       "          [ 0.4914,  0.1645],\n",
       "          [ 0.1627, -0.5318],\n",
       "          [-2.2035, -0.7041]],\n",
       "\n",
       "         [[ 1.3143, -1.1297],\n",
       "          [-1.7467, -0.7517],\n",
       "          [-0.7964,  0.3968],\n",
       "          [ 1.1945, -1.2584],\n",
       "          [ 0.0286,  0.8122]],\n",
       "\n",
       "         [[ 1.4008, -0.3317],\n",
       "          [-0.2632, -1.4176],\n",
       "          [-1.3153,  1.2273],\n",
       "          [ 0.1409,  1.1128],\n",
       "          [ 0.3915, -1.7735]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4330, -0.8483],\n",
       "          [-1.3166,  0.3196],\n",
       "          [ 0.4914,  0.1645],\n",
       "          [ 0.1627, -0.5318],\n",
       "          [-2.2035, -0.7041]],\n",
       "\n",
       "         [[ 1.3143, -1.1297],\n",
       "          [-1.7467, -0.7517],\n",
       "          [-0.7964,  0.3968],\n",
       "          [ 1.1945, -1.2584],\n",
       "          [ 0.0286,  0.8122]],\n",
       "\n",
       "         [[ 1.4008, -0.3317],\n",
       "          [-0.2632, -1.4176],\n",
       "          [-1.3153,  1.2273],\n",
       "          [ 0.1409,  1.1128],\n",
       "          [ 0.3915, -1.7735]]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
